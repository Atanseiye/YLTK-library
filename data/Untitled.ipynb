{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4022e060-4a28-4390-8bca-83951ec65208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "46ebaed0-733c-4797-a8da-9055dc47ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with gzip.open('yo_C3_large_clean_plus_noisy.txt.gz', 'rb') as f:\n",
    "    file_content = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a58a6322-786b-4e04-a56b-0782465caec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54525844"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = file_content\n",
    "# Decode it\n",
    "decoded_text = encoded_text.decode('utf-8')\n",
    "len(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a09fa4f3-8aab-4d65-8372-330fcc237208",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = load_dataset(\"mxronga/yoruba-proverbs-parallel-corpora\")\n",
    "data_1 = ' '.join(ds['train']['yoruba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "92ca0bd5-0a76-4591-9b11-eb07d8d0c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_int = load_dataset(\"saillab/alpaca-yoruba-cleaned\")\n",
    "data_2 = ' '.join(data_int['train']['instruction'])\n",
    "data_3 = ' '.join(data_int['train']['output'])\n",
    "data_4 = ' '.join(data_int['train']['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a625bf-cf9b-4aa3-9566-41948dd411c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the dataset\n",
    "url_1 = \"https://raw.githubusercontent.com/uds-lsv/transfer-distant-transformer-african/master/data/yoruba_newsclass/train_clean.tsv\"\n",
    "url_2 = \"https://raw.githubusercontent.com/uds-lsv/transfer-distant-transformer-african/master/data/yoruba_newsclass/dev.tsv\"\n",
    "url_3 = \"https://raw.githubusercontent.com/uds-lsv/transfer-distant-transformer-african/master/data/yoruba_newsclass/test.tsv\"\n",
    "\n",
    "# Load the TSV file into a DataFrame\n",
    "data_link_1 = pd.read_csv(url_1, sep='\\t')\n",
    "data_link_2 = pd.read_csv(url_2, sep='\\t')\n",
    "data_link_3 = pd.read_csv(url_3, sep='\\t')\n",
    "\n",
    "link_1 = data_link_1['news_title'].str.split(':', expand=True)[1]\n",
    "link_2 = data_link_2['news_title'].str.split(':', expand=True)[1]\n",
    "link_3 = data_link_3['news_title'].str.split(':', expand=True)[1]\n",
    "\n",
    "link_1.dropna(inplace=True)\n",
    "link_2.dropna(inplace=True)\n",
    "link_3.dropna(inplace=True)\n",
    "\n",
    "data_5 = ' '.join(link_1)\n",
    "data_6 = ' '.join(link_2)\n",
    "data_7 = ' '.join(link_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4a63a860-9adf-4421-8bf0-376c91291131",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_1 = pd.read_csv('train.tsv', sep='\\t', on_bad_lines='skip', engine='python')\n",
    "data_file_2 = pd.read_csv('test.tsv', sep='\\t', on_bad_lines='skip', engine='python')\n",
    "data_file_3 = pd.read_csv('dev.tsv', sep='\\t', on_bad_lines='skip', engine='python')\n",
    "\n",
    "data_8 = ' '.join(data_file_1['yo_review'])\n",
    "data_9 = ' '.join(data_file_2['yo_review'])\n",
    "data_10 = ' '.join(data_file_3['yo_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bf5380ad-a9e5-4f05-b114-4fc29fe86363",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hug_1 = load_dataset('DGurgurov/yoruba_sa')\n",
    "data_11 = ' '.join(dataset_hug_1['train']['text'])\n",
    "data_12 = ' '.join(dataset_hug_1['validation']['text'])\n",
    "data_13 = ' '.join(dataset_hug_1['test']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3cdb95b9-be07-4264-a4b2-3b33b467c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hug_2 = load_dataset('saillab/alpaca_yoruba_taco')\n",
    "data_14 = ' '.join(dataset_hug_2['train']['instruction'])\n",
    "data_15 = ' '.join(dataset_hug_2['test']['instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e245d4e3-3d8c-4b1f-86e9-faa64b02a4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89596083"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_7) + len(data_6) + len(data_5) + len(data_4) + len(data_3) + len(data_2) + len(data_1) + len(decoded_text) + len(data_8) + len(data_9) + len(data_10)  + len(data_11)  + len(data_12) + len(data_13)  + len(data_14) + len(data_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d5613604-38e1-4aae-9a64-308dc3c0a1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eacc81c-bf79-4fe5-85a3-8a893c837005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import Counter\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# Load the API key from the environment variable\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set!\")\n",
    "\n",
    "# Replace with your OpenAI API key\n",
    "openai.api_key = api_key\n",
    "\n",
    "class YLTK:\n",
    "    def __init__():\n",
    "        return 1\n",
    "    \n",
    "    # Extract all yoruba text in a word\n",
    "    def extract_yoruba_words(text, max_tokens=2000, max_workers=5):\n",
    "        \"\"\"\n",
    "        Extract Yoruba words from the given text using OpenAI GPT model in parallel batches.\n",
    "\n",
    "        :param text: The full text to process.\n",
    "        :param max_tokens: Maximum number of tokens per batch (default: 127,000).\n",
    "        :param max_workers: Maximum number of parallel workers (default: 5).\n",
    "        :return: Extracted Yoruba words across all batches.\n",
    "        \"\"\"\n",
    "        # Function to split text into batches of approximately max_tokens\n",
    "        def split_text_into_batches(text, max_tokens):\n",
    "            words = text.split()  # Split the text into words\n",
    "            batches = []\n",
    "            current_batch = []\n",
    "            current_tokens = 0\n",
    "\n",
    "            for word in words:\n",
    "                word_length = len(word) + 1  # Estimate token count for the word (+1 for space/punctuation)\n",
    "                if current_tokens + word_length > max_tokens:\n",
    "                    batches.append(\" \".join(current_batch))  # Add current batch to batches\n",
    "                    current_batch = []  # Reset for new batch\n",
    "                    current_tokens = 0\n",
    "                current_batch.append(word)\n",
    "                current_tokens += word_length\n",
    "\n",
    "            # Add the last batch if it has content\n",
    "            if current_batch:\n",
    "                batches.append(\" \".join(current_batch))\n",
    "\n",
    "            return batches\n",
    "\n",
    "        # Function to process a single batch\n",
    "        def process_batch(batch, batch_index, total_batches):\n",
    "            prompt = f\"\"\"\n",
    "            Extract all Yoruba words from the following text:\n",
    "            {batch}\n",
    "\n",
    "            Output only the Yoruba words and remove all special characters. No additional commas after each word if they were not there.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-4-turbo\",  # Use gpt-3.5-turbo for a cheaper option\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a Yoruba language expert.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0  # Ensures deterministic output\n",
    "                )\n",
    "                # Extract and return the content of the assistant's response\n",
    "                yoruba_words = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                print(f\"Batch {batch_index + 1}/{total_batches} processed successfully.\")\n",
    "                return yoruba_words\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {batch_index + 1}/{total_batches}: {e}\")\n",
    "                return \"\"  # Return an empty string for failed batches\n",
    "\n",
    "        # Split the input text into batches\n",
    "        batches = split_text_into_batches(text, max_tokens)\n",
    "        total_batches = len(batches)  # Calculate the total number of batches\n",
    "        print(f\"Total number of batches: {total_batches}\")\n",
    "\n",
    "        yoruba_words = []\n",
    "\n",
    "        # Use ThreadPoolExecutor for parallel processing\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all batches to the executor\n",
    "            future_to_batch = {executor.submit(process_batch, batch, i, total_batches): i for i, batch in enumerate(batches)}\n",
    "\n",
    "            # Collect results as they are completed\n",
    "            for future in as_completed(future_to_batch):\n",
    "                batch_index = future_to_batch[future]\n",
    "                try:\n",
    "                    yoruba_words_batch = future.result()\n",
    "                    yoruba_words.append(yoruba_words_batch)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error retrieving result for batch {batch_index + 1}: {e}\")\n",
    "\n",
    "        # Combine all Yoruba words from batches\n",
    "        return \" \".join(yoruba_words)\n",
    "\n",
    "    \n",
    "    # Getting the frequecy of words\n",
    "    def word_frequency_count(text):\n",
    "        import re\n",
    "        # Helper function to count word frequencies for a chunk of text\n",
    "        def count_words(text_chunk):\n",
    "            # Preprocess the text: remove punctuation, convert to lowercase\n",
    "            cleaned_text = re.sub(r'[^\\w\\s]', '', text_chunk).lower()\n",
    "\n",
    "            # Split the text into words\n",
    "            words = cleaned_text.split()\n",
    "\n",
    "            # Use Counter to count occurrences of each word\n",
    "            return Counter(words)\n",
    "\n",
    "        # Split the text into smaller chunks for parallel processing\n",
    "        chunk_size = len(text) // 4  # Adjust the number of chunks as needed\n",
    "        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "        # Use ThreadPoolExecutor to process the chunks in parallel\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(count_words, chunks))\n",
    "\n",
    "        # Combine all word counts from each chunk\n",
    "        total_word_counts = Counter()\n",
    "        for result in results:\n",
    "            total_word_counts.update(result)\n",
    "\n",
    "        return dict(total_word_counts)\n",
    "\n",
    "    # Generating the list of stop words\n",
    "    def stop_words(file: dict):\n",
    "        # Helper function to check if a word's frequency is greater than or equal to 5\n",
    "        def check_word(word_item):\n",
    "            word, count = word_item\n",
    "            if count >= 5:\n",
    "                return word\n",
    "            return None\n",
    "\n",
    "        # Get the maximum number of workers (CPU cores)\n",
    "        max_workers = os.cpu_count()\n",
    "\n",
    "        # Use ThreadPoolExecutor to process the word count items in parallel\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            results = list(executor.map(check_word, file.items()))\n",
    "\n",
    "        # Filter out the None values (words that don't meet the condition)\n",
    "        list_of_stopWords = [word for word in results if word is not None]\n",
    "\n",
    "        return list_of_stopWords\n",
    "    \n",
    "    def remove_stopwords(list_a, document_b):\n",
    "        \"\"\"\n",
    "        Removes any text in list_a from document_b and returns the remaining text in document_b.\n",
    "\n",
    "        :param list_a: List of strings to be removed\n",
    "        :param document_b: The document (string) from which the text will be removed\n",
    "        :return: The remaining text in document_b as a string\n",
    "        \"\"\"\n",
    "        # Convert document_b into a list of words\n",
    "        document_b_words = document_b.split()\n",
    "\n",
    "        # Create a set from list_a for efficient lookup\n",
    "        list_a_set = set(list_a)\n",
    "\n",
    "        # Filter words in document_b that are not in list_a\n",
    "        remaining_words = [word for word in document_b_words if word not in list_a_set]\n",
    "\n",
    "        # Join the remaining words back into a single string\n",
    "        return \" \".join(remaining_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a9aa744-a65d-46bf-b9e0-1325f4adbaf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches: 5\n",
      "Batch 5/5 processed successfully.\n",
      "Batch 3/5 processed successfully.\n",
      "Batch 2/5 processed successfully.\n",
      "Batch 4/5 processed successfully.\n",
      "Batch 1/5 processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def process_1(data_batch):\n",
    "    return YLTK.extract_yoruba_words(data_batch)\n",
    "\n",
    "def process_2(word_batch):\n",
    "    return YLTK.word_frequency_count(word_batch)\n",
    "\n",
    "def process_3(word_count_batch):\n",
    "    return YLTK.stop_words(word_count_batch)\n",
    "\n",
    "def pipeline(data):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_1 = executor.submit(process_1, data)\n",
    "        result_1 = future_1.result()  # Get the result of process 1\n",
    "        future_2 = executor.submit(process_2, result_1)  # Process 2 starts as soon as process 1 is done\n",
    "        result_2 = future_2.result()  # Get the result of process 2\n",
    "        future_3 = executor.submit(process_3, result_2)  # Process 3 starts as soon as process 2 is done\n",
    "        result_3 = future_3.result()  # Get the final result\n",
    "        \n",
    "    # Write the list to a text file\n",
    "    with open('stop_words.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(str(result_3))  # Converts the list to a string and writes it to the file\n",
    "        \n",
    "\n",
    "        \n",
    "    return 'Processes successful!'\n",
    "\n",
    "\n",
    "final_result = pipeline(data_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44838992-f3b3-4c9e-9757-c95e25b0f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_stopwords = YLTK.remove_stopwords(stop_words, cleaning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
